{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jia5UcZuUGOK"
      },
      "source": [
        "# Ingenier√≠a de Caracter√≠sticas para Mantenimiento Predictivo\n",
        "## Proyecto: Predicci√≥n de Fallas en Moto-Compresores - Oil & Gas\n",
        "\n",
        "### üéØ Objetivo del Notebook\n",
        "\n",
        "Este notebook constituye la **fase cr√≠tica** de transformaci√≥n de datos donde convertimos las series temporales limpias en un dataset enriquecido y etiquetado, optimizado para el entrenamiento de modelos de Machine Learning. Nuestro objetivo principal es **predecir fallas en moto-compresores con 7 d√≠as de antelaci√≥n**, una ventana temporal que permite la planificaci√≥n efectiva de mantenimientos preventivos en el sector Oil & Gas.\n",
        "\n",
        "### üìã Tareas Principales\n",
        "\n",
        "1. **Carga y Validaci√≥n de Datos**: Integrar el dataset preprocesado con el historial de eventos\n",
        "2. **Ingenier√≠a de Caracter√≠sticas Temporales**: Crear features que capturen la din√°mica del deterioro\n",
        "3. **Caracter√≠sticas Avanzadas**: Implementar features de tasas de cambio, frecuencia y detecci√≥n de anomal√≠as\n",
        "4. **Etiquetado de Fallas**: Crear la variable objetivo basada en ventanas de pre-falla de 7 d√≠as\n",
        "5. **Validaci√≥n y Preparaci√≥n Final**: Garantizar calidad de datos para modelado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j_9n3AhRUGOM",
        "outputId": "c083b6cd-e470-4d66-e6c0-501be7e6f88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Librer√≠as importadas exitosamente\n",
            "üìä Versi√≥n de pandas: 2.2.2\n",
            "üî¢ Versi√≥n de numpy: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "# Importaci√≥n de librer√≠as esenciales para ingenier√≠a de caracter√≠sticas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "\n",
        "# Librer√≠as especializadas para an√°lisis de se√±ales y anomal√≠as\n",
        "from scipy import signal\n",
        "from scipy.fft import fft, fftfreq\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Configuraci√≥n del entorno\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
        "print(f\"üìä Versi√≥n de pandas: {pd.__version__}\")\n",
        "print(f\"üî¢ Versi√≥n de numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"üìÇ Creando estructura de directorios...\")\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('eventos', exist_ok=True)\n",
        "print(\"‚úÖ Carpetas listas: 'data/processed' y 'eventos'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu3m0w4PHfmw",
        "outputId": "5b955055-1d1e-4057-cf66-f849eafd3777"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Creando estructura de directorios...\n",
            "‚úÖ Carpetas listas: 'data/processed' y 'eventos'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xCh59IUuUGON",
        "outputId": "959f3f3b-2d3c-4d5d-9eeb-370810ef873b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Validaci√≥n de rutas y archivos:\n",
            "   Datos procesados: data/processed - ‚úÖ Existe\n",
            "   Eventos: eventos - ‚úÖ Existe\n",
            "   Timeseries: data/processed/timeseries_data.parquet - ‚úÖ Existe\n",
            "   Historial: eventos/Historial C1 RGD.xlsx - ‚úÖ Existe\n",
            "\n",
            "‚úÖ Todos los archivos requeridos est√°n disponibles\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n de rutas de datos con validaci√≥n de existencia\n",
        "# Esta configuraci√≥n garantiza la reproducibilidad del pipeline\n",
        "\n",
        "# Directorio base del proyecto\n",
        "base_dir = Path('.')\n",
        "\n",
        "# Rutas espec√≠ficas para datos procesados y eventos\n",
        "ruta_processed = base_dir / 'data' / 'processed'\n",
        "ruta_eventos = base_dir / 'eventos'\n",
        "\n",
        "# Archivos espec√≠ficos requeridos\n",
        "archivo_timeseries = ruta_processed / 'timeseries_data.parquet'\n",
        "archivo_historial = ruta_eventos / 'Historial C1 RGD.xlsx'\n",
        "\n",
        "# Validaci√≥n cr√≠tica de existencia de archivos\n",
        "print(\"üìÅ Validaci√≥n de rutas y archivos:\")\n",
        "print(f\"   Datos procesados: {ruta_processed} - {'‚úÖ Existe' if ruta_processed.exists() else '‚ùå No existe'}\")\n",
        "print(f\"   Eventos: {ruta_eventos} - {'‚úÖ Existe' if ruta_eventos.exists() else '‚ùå No existe'}\")\n",
        "print(f\"   Timeseries: {archivo_timeseries} - {'‚úÖ Existe' if archivo_timeseries.exists() else '‚ùå No existe'}\")\n",
        "print(f\"   Historial: {archivo_historial} - {'‚úÖ Existe' if archivo_historial.exists() else '‚ùå No existe'}\")\n",
        "\n",
        "# Verificaci√≥n cr√≠tica - detener ejecuci√≥n si faltan archivos esenciales\n",
        "archivos_requeridos = [archivo_timeseries, archivo_historial]\n",
        "archivos_faltantes = [arch for arch in archivos_requeridos if not arch.exists()]\n",
        "\n",
        "if archivos_faltantes:\n",
        "    print(f\"\\n‚ùå ERROR CR√çTICO: Faltan archivos esenciales:\")\n",
        "    for archivo in archivos_faltantes:\n",
        "        print(f\"   - {archivo}\")\n",
        "    raise FileNotFoundError(\"No se pueden continuar sin los archivos de datos requeridos\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Todos los archivos requeridos est√°n disponibles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3mi3Vy7UGOO"
      },
      "source": [
        "## 1. üìÇ Carga y Validaci√≥n de Datos\n",
        "\n",
        "### üîÑ Proceso de Carga Inteligente\n",
        "\n",
        "En esta fase cr√≠tica, cargaremos tanto el **dataset de series temporales procesado** como el **historial de eventos de mantenimiento**. La calidad de este proceso determina directamente la efectividad de nuestro modelo predictivo.\n",
        "\n",
        "El dataset de series temporales contiene las mediciones continuas de sensores del moto-compresor, ya limpias y preprocesadas. El historial de eventos proporciona las fechas exactas de las fallas hist√≥ricas, informaci√≥n esencial para crear nuestras etiquetas de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KCMEyh4KUGOO",
        "outputId": "8028b7cc-4b65-46e3-9a9f-7bfee9d252ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Cargando dataset principal de series temporales...\n",
            "                     pres_aceite_comp          rpm  presion_aceite_motor  presion_agua  presion_carter  temp_aceite_motor  temp_agua_motor  temp_mult_adm_izq\n",
            "hora                                                                                                                                                         \n",
            "2023-01-01 00:00:00         60.995666  1099.724817             57.188761     15.238856        0.540845          89.621252        86.009180          57.552369\n",
            "2023-01-01 01:00:00         60.244113  1100.752074             55.039625     15.134238        0.549441          90.526367        84.226618          60.181223\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 20401 entries, 2023-01-01 00:00:00 to 2025-04-30 00:00:00\n",
            "Data columns (total 8 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   pres_aceite_comp      20401 non-null  float64\n",
            " 1   rpm                   20401 non-null  float64\n",
            " 2   presion_aceite_motor  20401 non-null  float64\n",
            " 3   presion_agua          20401 non-null  float64\n",
            " 4   presion_carter        20401 non-null  float64\n",
            " 5   temp_aceite_motor     20401 non-null  float64\n",
            " 6   temp_agua_motor       20401 non-null  float64\n",
            " 7   temp_mult_adm_izq     20401 non-null  float64\n",
            "dtypes: float64(8)\n",
            "memory usage: 1.4 MB\n",
            "None\n",
            "üîß Ordenando el DataFrame por el √≠ndice de tiempo...\n",
            "‚úÖ El √≠ndice est√° ordenado correctamente.\n",
            "‚úÖ √çndice principal ya es √∫nico.\n",
            "‚úÖ Dataset principal cargado exitosamente\n",
            "   üìä Dimensiones: 20,401 filas √ó 8 columnas\n",
            "   üìÖ Per√≠odo temporal: 2023-01-01 00:00:00 ‚Üí 2025-04-30 00:00:00\n",
            "   ‚è±Ô∏è  Frecuencia detectada: h\n",
            "   üíæ Uso de memoria: 1.4 MB\n",
            "   üìà Completitud promedio: 100.0%\n",
            "   üî¢ Tipos de datos: {dtype('float64'): np.int64(8)}\n"
          ]
        }
      ],
      "source": [
        "# Carga optimizada del dataset principal de series temporales\n",
        "print(\"‚ö° Cargando dataset principal de series temporales...\")\n",
        "\n",
        "try:\n",
        "    # Cargar el dataset principal con optimizaciones de memoria\n",
        "    df = pd.read_parquet(archivo_timeseries, engine='pyarrow')\n",
        "    # Convertir la columna 'hora' en DatetimeIndex\n",
        "    if 'hora' in df.columns:\n",
        "        df['hora'] = pd.to_datetime(df['hora'])\n",
        "        df.set_index('hora', inplace=True)\n",
        "    print(df.head(2))\n",
        "    print(df.info())\n",
        "\n",
        "    # Ordenar el √≠ndice del DataFrame por si acaso no est√° en orden cronol√≥gico\n",
        "    print(\"üîß Ordenando el DataFrame por el √≠ndice de tiempo...\")\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # 3. (Opcional pero recomendado) Verifica que el √≠ndice ya est√° ordenado\n",
        "    if df.index.is_monotonic_increasing:\n",
        "        print(\"‚úÖ El √≠ndice est√° ordenado correctamente.\")\n",
        "    else:\n",
        "        # Esto no deber√≠a pasar despu√©s de sort_index(), pero es una buena comprobaci√≥n\n",
        "        print(\"‚ö†Ô∏è El √≠ndice A√öN no est√° ordenado. Revisa la fuente de datos.\")\n",
        "\n",
        "    # --- LIMPIEZA DEL DATAFRAME PRINCIPAL ---\n",
        "    if not df.index.is_unique:\n",
        "        duplicados_encontrados = df.index.duplicated().sum()\n",
        "        print(f\"‚ö†Ô∏è  √çndice principal con {duplicados_encontrados} duplicados. Limpiando...\")\n",
        "        df = df[~df.index.duplicated(keep='first')]\n",
        "        print(f\"‚úÖ √çndice principal limpio. Total filas ahora: {len(df)}\")\n",
        "    else:\n",
        "        print(\"‚úÖ √çndice principal ya es √∫nico.\")\n",
        "\n",
        "    # Validaciones cr√≠ticas inmediatas\n",
        "    if df.empty:\n",
        "        raise ValueError(\"El dataset cargado est√° vac√≠o\")\n",
        "\n",
        "    \"\"\" if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        print(\"‚ö†Ô∏è  Convirtiendo √≠ndice a DatetimeIndex\")\n",
        "        df.index = pd.to_datetime(df.index) \"\"\"\n",
        "\n",
        "    # Informaci√≥n b√°sica del dataset\n",
        "    print(f\"‚úÖ Dataset principal cargado exitosamente\")\n",
        "    print(f\"   üìä Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
        "    print(f\"   üìÖ Per√≠odo temporal: {df.index.min()} ‚Üí {df.index.max()}\")\n",
        "    print(f\"   ‚è±Ô∏è  Frecuencia detectada: {pd.infer_freq(df.index) or 'Variable/No detectada'}\")\n",
        "    print(f\"   üíæ Uso de memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "    # An√°lisis de calidad de datos\n",
        "    completitud_promedio = (df.count().sum() / (df.shape[0] * df.shape[1])) * 100\n",
        "    print(f\"   üìà Completitud promedio: {completitud_promedio:.1f}%\")\n",
        "\n",
        "    # Verificar tipos de datos\n",
        "    tipos_datos = df.dtypes.value_counts()\n",
        "    print(f\"   üî¢ Tipos de datos: {dict(tipos_datos)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cr√≠tico al cargar el dataset principal: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "va2cf_prUGOP",
        "outputId": "0e878f7e-8e5d-45f8-f3fa-e77c2c3cac18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Cargando historial de eventos de mantenimiento (FORZADO)...\n",
            "üìä Dimensiones crudas: (9, 3)\n",
            "   üìÖ Columna de fecha detectada: 0 (8 fechas v√°lidas)\n",
            "‚úÖ Historial procesado exitosamente\n",
            "   üìÖ Eventos v√°lidos: 8\n",
            "   üìã Columnas: ['fecha_evento', 'descripcion_falla', 'equipo']\n",
            "   üîç Ejemplo: 2023-03-15 00:00:00 - Falla: Fuga de Refrigerante\n"
          ]
        }
      ],
      "source": [
        "# Carga y procesamiento del historial de eventos - VERSI√ìN BLINDADA\n",
        "print(\"\\nüîÑ Cargando historial de eventos de mantenimiento (FORZADO)...\")\n",
        "\n",
        "def cargar_historial_eventos_blindado(archivo_historial):\n",
        "    try:\n",
        "        # 1. Cargar SIN cabecera para no perder la primera fila de datos\n",
        "        df_raw = pd.read_excel(archivo_historial, header=None, engine='openpyxl')\n",
        "\n",
        "        print(f\"üìä Dimensiones crudas: {df_raw.shape}\")\n",
        "\n",
        "        # 2. Identificar columnas clave autom√°ticamente\n",
        "        # Buscamos columnas que parezcan fechas\n",
        "        col_fecha = None\n",
        "        col_desc = None\n",
        "\n",
        "        for col in df_raw.columns:\n",
        "            # Verificar si la columna es tipo fecha o parece fecha\n",
        "            try:\n",
        "                temp_date = pd.to_datetime(df_raw[col], errors='coerce')\n",
        "                validas = temp_date.notna().sum()\n",
        "                if validas > 0.5 * len(df_raw): # Si m√°s del 50% son fechas\n",
        "                    col_fecha = col\n",
        "                    print(f\"   üìÖ Columna de fecha detectada: {col} ({validas} fechas v√°lidas)\")\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Si no detectamos fecha, asumimos la primera columna\n",
        "        if col_fecha is None:\n",
        "            col_fecha = 0\n",
        "            print(\"   ‚ö†Ô∏è No se detect√≥ columna de fecha, asumiendo columna 0\")\n",
        "\n",
        "        # 3. Renombrar columnas manualmente (Est√°ndar)\n",
        "        # Asumimos estructura: [FECHA, DESCRIPCI√ìN, EQUIPO] o similar\n",
        "        nuevas_columnas = {\n",
        "            col_fecha: 'fecha_evento',\n",
        "            col_fecha + 1: 'descripcion_falla' if (col_fecha + 1) in df_raw.columns else 'col_1',\n",
        "            col_fecha + 2: 'equipo' if (col_fecha + 2) in df_raw.columns else 'col_2'\n",
        "        }\n",
        "\n",
        "        df_eventos = df_raw.rename(columns=nuevas_columnas)\n",
        "\n",
        "        # 4. Limpieza final\n",
        "        df_eventos['fecha_evento'] = pd.to_datetime(df_eventos['fecha_evento'], errors='coerce')\n",
        "        df_eventos = df_eventos.dropna(subset=['fecha_evento'])\n",
        "\n",
        "        print(f\"‚úÖ Historial procesado exitosamente\")\n",
        "        print(f\"   üìÖ Eventos v√°lidos: {len(df_eventos)}\")\n",
        "        print(f\"   üìã Columnas: {list(df_eventos.columns)}\")\n",
        "        print(f\"   üîç Ejemplo: {df_eventos.iloc[0]['fecha_evento']} - {df_eventos.iloc[0].get('descripcion_falla', 'N/A')}\")\n",
        "\n",
        "        return df_eventos\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error cargando historial: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Ejecutar\n",
        "df_eventos = cargar_historial_eventos_blindado(archivo_historial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MoEySwh7UGOQ",
        "outputId": "662f43aa-4c35-4721-80f9-648cba4d2c8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Validando compatibilidad temporal entre datasets...\n",
            "üìä An√°lisis de cobertura temporal:\n",
            "   üîß Datos de sensores: 2023-01-01 00:00:00 a 2025-04-30 00:00:00\n",
            "   üìÖ Eventos registrados: 2023-03-15 00:00:00 a 2025-03-01 00:00:00\n",
            "   ‚úÖ Solapamiento detectado: 2023-03-15 00:00:00 a 2025-03-01 00:00:00\n",
            "   ‚è±Ô∏è  Duraci√≥n del solapamiento: 717 days 00:00:00\n",
            "   üéØ Eventos utilizables para entrenamiento: 8\n",
            "   ‚úÖ COMPATIBILIDAD TEMPORAL CONFIRMADA\n",
            "   üìà Distribuci√≥n anual de eventos: {2023: np.int64(4), 2024: np.int64(3), 2025: np.int64(1)}\n",
            "   üìä Densidad de eventos: 0.0094 eventos/d√≠a\n",
            "\n",
            "üìã RESUMEN DE VALIDACI√ìN TEMPORAL:\n",
            "   Compatibilidad temporal: ‚úÖ CONFIRMADA\n",
            "   Eventos utilizables: 8\n",
            "   Duraci√≥n de solapamiento: 717 days 00:00:00\n",
            "\n",
            "üéØ Listo para proceder con ingenier√≠a de caracter√≠sticas\n"
          ]
        }
      ],
      "source": [
        "# Validaci√≥n de compatibilidad temporal entre datasets - CORREGIDO\n",
        "print(\"\\nüîç Validando compatibilidad temporal entre datasets...\")\n",
        "\n",
        "# Inicializar variables de control\n",
        "eventos_utilizables = None\n",
        "overlap_duration = None\n",
        "compatibilidad_temporal = False\n",
        "\n",
        "try:\n",
        "    # Validar pre-requisitos\n",
        "    if df is None or df.empty:\n",
        "        print(\"   ‚ùå Dataset principal no disponible\")\n",
        "    elif df_eventos is None or df_eventos.empty:\n",
        "        print(\"   ‚ùå Dataset de eventos no disponible\")\n",
        "    elif not isinstance(df.index, pd.DatetimeIndex):\n",
        "        print(\"   ‚ùå √çndice del dataset principal no es de tipo fecha\")\n",
        "    elif 'fecha_evento' not in df_eventos.columns:\n",
        "        print(\"   ‚ùå No se encontr√≥ columna 'fecha_evento' en el historial\")\n",
        "    else:\n",
        "        # An√°lisis de rangos temporales\n",
        "        sensor_start, sensor_end = df.index.min(), df.index.max()\n",
        "        eventos_limpios = df_eventos['fecha_evento'].dropna()\n",
        "\n",
        "        if eventos_limpios.empty:\n",
        "            print(\"   ‚ùå No hay eventos con fechas v√°lidas\")\n",
        "        else:\n",
        "            eventos_start, eventos_end = eventos_limpios.min(), eventos_limpios.max()\n",
        "\n",
        "            print(f\"üìä An√°lisis de cobertura temporal:\")\n",
        "            print(f\"   üîß Datos de sensores: {sensor_start} a {sensor_end}\")\n",
        "            print(f\"   üìÖ Eventos registrados: {eventos_start} a {eventos_end}\")\n",
        "\n",
        "            # Calcular solapamiento temporal\n",
        "            overlap_start = max(sensor_start, eventos_start)\n",
        "            overlap_end = min(sensor_end, eventos_end)\n",
        "\n",
        "            if overlap_start <= overlap_end:\n",
        "                overlap_duration = overlap_end - overlap_start\n",
        "                print(f\"   ‚úÖ Solapamiento detectado: {overlap_start} a {overlap_end}\")\n",
        "                print(f\"   ‚è±Ô∏è  Duraci√≥n del solapamiento: {overlap_duration}\")\n",
        "\n",
        "                # Eventos que caen dentro del rango de datos de sensores\n",
        "                eventos_utilizables = eventos_limpios[\n",
        "                    (eventos_limpios >= sensor_start) & (eventos_limpios <= sensor_end)\n",
        "                ]\n",
        "\n",
        "                print(f\"   üéØ Eventos utilizables para entrenamiento: {len(eventos_utilizables)}\")\n",
        "\n",
        "                if len(eventos_utilizables) > 0:\n",
        "                    compatibilidad_temporal = True\n",
        "                    print(f\"   ‚úÖ COMPATIBILIDAD TEMPORAL CONFIRMADA\")\n",
        "\n",
        "                    # An√°lisis de distribuci√≥n temporal de eventos\n",
        "                    eventos_por_year = eventos_utilizables.dt.year.value_counts().sort_index()\n",
        "                    print(f\"   üìà Distribuci√≥n anual de eventos: {dict(eventos_por_year)}\")\n",
        "\n",
        "                    # Estad√≠sticas de densidad de eventos\n",
        "                    dias_totales = (sensor_end - sensor_start).days\n",
        "                    densidad_eventos = len(eventos_utilizables) / dias_totales if dias_totales > 0 else 0\n",
        "                    print(f\"   üìä Densidad de eventos: {densidad_eventos:.4f} eventos/d√≠a\")\n",
        "                else:\n",
        "                    print(f\"   ‚ö†Ô∏è  Sin eventos utilizables dentro del rango de sensores\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå No hay solapamiento temporal entre datasets\")\n",
        "                print(f\"       Sensores terminan: {sensor_end}\")\n",
        "                print(f\"       Eventos inician: {eventos_start}\")\n",
        "\n",
        "                # An√°lisis de eventos fuera de rango\n",
        "                eventos_antes = eventos_limpios[eventos_limpios < sensor_start]\n",
        "                eventos_despues = eventos_limpios[eventos_limpios > sensor_end]\n",
        "                print(f\"       Eventos antes del rango: {len(eventos_antes)}\")\n",
        "                print(f\"       Eventos despu√©s del rango: {len(eventos_despues)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error en validaci√≥n temporal: {str(e)}\")\n",
        "    compatibilidad_temporal = False\n",
        "\n",
        "# Resumen ejecutivo de la validaci√≥n\n",
        "print(f\"\\nüìã RESUMEN DE VALIDACI√ìN TEMPORAL:\")\n",
        "print(f\"   Compatibilidad temporal: {'‚úÖ CONFIRMADA' if compatibilidad_temporal else '‚ùå NO CONFIRMADA'}\")\n",
        "print(f\"   Eventos utilizables: {len(eventos_utilizables) if eventos_utilizables is not None else 0}\")\n",
        "print(f\"   Duraci√≥n de solapamiento: {overlap_duration if overlap_duration else 'N/A'}\")\n",
        "\n",
        "# Configurar variables para las siguientes etapas\n",
        "if not compatibilidad_temporal:\n",
        "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Sin compatibilidad temporal, se proceder√° con an√°lisis limitado\")\n",
        "    eventos_utilizables = pd.Series(dtype='datetime64[ns]')  # Serie vac√≠a\n",
        "else:\n",
        "    print(f\"\\nüéØ Listo para proceder con ingenier√≠a de caracter√≠sticas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXWYSAQYUGOR"
      },
      "source": [
        "## 2. üèóÔ∏è Ingenier√≠a de Caracter√≠sticas Temporales\n",
        "\n",
        "### üìà Caracter√≠sticas de Ventanas M√≥viles (Rolling Features)\n",
        "\n",
        "Las caracter√≠sticas de ventanas m√≥viles son fundamentales para capturar la **din√°mica temporal del deterioro** en equipos industriales. Implementaremos m√∫ltiples horizontes temporales para detectar tanto degradaciones lentas como cambios s√∫bitos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "env0go3cUGOR",
        "outputId": "cf3f5e8c-4573-4ed8-d0d4-42f723b34f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèóÔ∏è INICIANDO CREACI√ìN DE CARACTER√çSTICAS TEMPORALES\n",
            "============================================================\n",
            "üìà Creando caracter√≠sticas de ventanas m√≥viles...\n",
            "   üìä Variables a procesar: 8\n",
            "   ‚è±Ô∏è  Ventanas temporales: ['6H', '24H', '72H']\n",
            "   üìã Estad√≠sticas: ['mean', 'std', 'min', 'max']\n",
            "\n",
            "   üîÑ Procesando ventana 6H...\n",
            "\n",
            "   üîÑ Procesando ventana 24H...\n",
            "\n",
            "   üîÑ Procesando ventana 72H...\n",
            "‚úÖ Rolling features creadas: 96\n",
            "üìê Dimensiones: 20,401 √ó 96\n",
            "\n",
            "üíæ Memoria rolling features: 15.1 MB\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n para crear caracter√≠sticas de ventanas m√≥viles optimizada\n",
        "def crear_rolling_features(df, ventanas=['6H', '24H', '72H'],\n",
        "                          estadisticas=['mean', 'std', 'min', 'max'],\n",
        "                          variables_prioritarias=None, max_features=200):\n",
        "    \"\"\"\n",
        "    Crea caracter√≠sticas de ventanas m√≥viles para m√∫ltiples estad√≠sticas\n",
        "\n",
        "    Par√°metros:\n",
        "    - df: DataFrame con series temporales\n",
        "    - ventanas: Lista de ventanas temporales (ej: ['6H', '24H'])\n",
        "    - estadisticas: Lista de estad√≠sticas a calcular\n",
        "    - variables_prioritarias: Variables espec√≠ficas a procesar\n",
        "    - max_features: L√≠mite m√°ximo de features a crear\n",
        "    \"\"\"\n",
        "    print(\"üìà Creando caracter√≠sticas de ventanas m√≥viles...\")\n",
        "\n",
        "    if variables_prioritarias is None:\n",
        "        # Seleccionar autom√°ticamente variables num√©ricas\n",
        "        variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        variables_prioritarias = variables_numericas[:20]  # Primeras 20\n",
        "\n",
        "    print(f\"   üìä Variables a procesar: {len(variables_prioritarias)}\")\n",
        "    print(f\"   ‚è±Ô∏è  Ventanas temporales: {ventanas}\")\n",
        "    print(f\"   üìã Estad√≠sticas: {estadisticas}\")\n",
        "\n",
        "    rolling_features = pd.DataFrame(index=df.index)\n",
        "    contador_features = 0\n",
        "\n",
        "    for ventana in ventanas:\n",
        "        print(f\"\\n   üîÑ Procesando ventana {ventana}...\")\n",
        "\n",
        "        for variable in variables_prioritarias:\n",
        "            if contador_features >= max_features:\n",
        "                print(f\"   ‚ö†Ô∏è  L√≠mite de {max_features} features alcanzado\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Crear rolling window\n",
        "                rolling = df[variable].rolling(window=ventana, min_periods=1)\n",
        "\n",
        "                for stat in estadisticas:\n",
        "                    if contador_features >= max_features:\n",
        "                        break\n",
        "\n",
        "                    nombre_feature = f\"{variable}_roll_{ventana}_{stat}\"\n",
        "\n",
        "                    if stat == 'mean':\n",
        "                        rolling_features[nombre_feature] = rolling.mean()\n",
        "                    elif stat == 'std':\n",
        "                        rolling_features[nombre_feature] = rolling.std()\n",
        "                    elif stat == 'min':\n",
        "                        rolling_features[nombre_feature] = rolling.min()\n",
        "                    elif stat == 'max':\n",
        "                        rolling_features[nombre_feature] = rolling.max()\n",
        "                    elif stat == 'median':\n",
        "                        rolling_features[nombre_feature] = rolling.median()\n",
        "                    elif stat == 'skew':\n",
        "                        rolling_features[nombre_feature] = rolling.skew()\n",
        "                    elif stat == 'kurt':\n",
        "                        rolling_features[nombre_feature] = rolling.kurt()\n",
        "\n",
        "                    contador_features += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      ‚ö†Ô∏è  Error con {variable}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if contador_features >= max_features:\n",
        "            break\n",
        "\n",
        "    print(f\"‚úÖ Rolling features creadas: {contador_features}\")\n",
        "    print(f\"üìê Dimensiones: {rolling_features.shape[0]:,} √ó {rolling_features.shape[1]}\")\n",
        "\n",
        "    return rolling_features\n",
        "\n",
        "# Ejecutar creaci√≥n de rolling features\n",
        "print(\"\\nüèóÔ∏è INICIANDO CREACI√ìN DE CARACTER√çSTICAS TEMPORALES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    rolling_features = crear_rolling_features(\n",
        "        df=df,\n",
        "        ventanas=['6H', '24H', '72H'],\n",
        "        estadisticas=['mean', 'std', 'min', 'max'],\n",
        "        max_features=150\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüíæ Memoria rolling features: {rolling_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creando rolling features: {str(e)}\")\n",
        "    rolling_features = pd.DataFrame(index=df.index)  # DataFrame vac√≠o como fallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlK6e7UVUGOS"
      },
      "source": [
        "### ‚è™ Caracter√≠sticas de Lag (Retrasos Temporales)\n",
        "\n",
        "Los features de lag son esenciales para **modelar la memoria temporal** del sistema, permitiendo al modelo acceder a estados hist√≥ricos del equipo para predecir comportamientos futuros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KM1YvhUkUGOT",
        "outputId": "91d3703c-b816-42bb-81f9-8a9930ee1ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚è™ INICIANDO CREACI√ìN DE LAG FEATURES CORREGIDA\n",
            "============================================================\n",
            "‚è™ Creando caracter√≠sticas de lag (retrasos temporales) - VERSI√ìN CORREGIDA...\n",
            "   üìä Variables a procesar: 8\n",
            "   ‚è±Ô∏è  Lags temporales: ['2H', '12H', '48H']\n",
            "   üîç Analizando frecuencia temporal del dataset...\n",
            "   ‚è±Ô∏è  Frecuencia detectada: h\n",
            "   ‚è±Ô∏è  Frecuencia promedio: 0 days 01:00:00\n",
            "\n",
            "   üîÑ Procesando lag 2H...\n",
            "      ‚ö†Ô∏è  M√©todo 1 fall√≥, usando promedio: 2H = 2 per√≠odos\n",
            "      ‚úÖ Features creados para 2H: 8\n",
            "\n",
            "   üîÑ Procesando lag 12H...\n",
            "      ‚ö†Ô∏è  M√©todo 1 fall√≥, usando promedio: 12H = 12 per√≠odos\n",
            "      ‚úÖ Features creados para 12H: 8\n",
            "\n",
            "   üîÑ Procesando lag 48H...\n",
            "      ‚ö†Ô∏è  M√©todo 1 fall√≥, usando promedio: 48H = 48 per√≠odos\n",
            "      ‚úÖ Features creados para 48H: 8\n",
            "\n",
            "   üìä Creando caracter√≠sticas de diferencias...\n",
            "      ‚úÖ Caracter√≠sticas de diferencias creadas: 16\n",
            "\n",
            "   üßπ Aplicando limpieza final...\n",
            "‚úÖ Lag features creadas exitosamente: 40 features totales\n",
            "üìê Dimensiones finales: 20,401 √ó 40\n",
            "üìà Completitud promedio: 99.9%\n",
            "\n",
            "üíæ Memoria lag features: 6.4 MB\n",
            "üéØ LAG FEATURES CREADAS EXITOSAMENTE\n"
          ]
        }
      ],
      "source": [
        "# Funci√≥n para crear caracter√≠sticas de lag CORREGIDA\n",
        "def crear_lag_features(df, lags=['2H', '12H', '48H'],\n",
        "                      variables_prioritarias=None, max_features=100):\n",
        "    \"\"\"\n",
        "    Crea caracter√≠sticas de lag (retrasos temporales) con manejo robusto de errores\n",
        "\n",
        "    CORRECCI√ìN: Calcula per√≠odos enteros bas√°ndose en la frecuencia del √≠ndice\n",
        "    para evitar el error \"unit abbreviation w/o a number\"\n",
        "    \"\"\"\n",
        "    print(\"‚è™ Creando caracter√≠sticas de lag (retrasos temporales) - VERSI√ìN CORREGIDA...\")\n",
        "\n",
        "    if variables_prioritarias is None:\n",
        "        variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        variables_prioritarias = variables_numericas[:12]  # Primeras 12\n",
        "\n",
        "    print(f\"   üìä Variables a procesar: {len(variables_prioritarias)}\")\n",
        "    print(f\"   ‚è±Ô∏è  Lags temporales: {lags}\")\n",
        "\n",
        "    # Detectar frecuencia del √≠ndice\n",
        "    print(f\"   üîç Analizando frecuencia temporal del dataset...\")\n",
        "    freq_detectada = pd.infer_freq(df.index)\n",
        "\n",
        "    # Calcular diferencia temporal promedio como fallback\n",
        "    if len(df.index) > 1:\n",
        "        time_diffs = df.index[1:] - df.index[:-1]\n",
        "        freq_promedio = time_diffs.median()\n",
        "        print(f\"   ‚è±Ô∏è  Frecuencia detectada: {freq_detectada}\")\n",
        "        print(f\"   ‚è±Ô∏è  Frecuencia promedio: {freq_promedio}\")\n",
        "    else:\n",
        "        freq_promedio = pd.Timedelta('1H')  # Default\n",
        "        print(f\"   ‚ö†Ô∏è  Usando frecuencia por defecto: {freq_promedio}\")\n",
        "\n",
        "    lag_features = pd.DataFrame(index=df.index)\n",
        "    contador_features = 0\n",
        "\n",
        "    for lag_str in lags:\n",
        "        print(f\"\\n   üîÑ Procesando lag {lag_str}...\")\n",
        "\n",
        "        try:\n",
        "            # CORRECCI√ìN PRINCIPAL: Convertir lag string a timedelta y calcular per√≠odos\n",
        "            lag_timedelta = pd.Timedelta(lag_str)\n",
        "\n",
        "            # M√©todo 1: Usar frecuencia detectada\n",
        "            if freq_detectada:\n",
        "                try:\n",
        "                    freq_td = pd.Timedelta(freq_detectada)\n",
        "                    lag_periods = int(lag_timedelta / freq_td)\n",
        "                    print(f\"      ‚úÖ M√©todo 1 exitoso: {lag_str} = {lag_periods} per√≠odos (freq: {freq_detectada})\")\n",
        "                except:\n",
        "                    lag_periods = int(lag_timedelta / freq_promedio)\n",
        "                    print(f\"      ‚ö†Ô∏è  M√©todo 1 fall√≥, usando promedio: {lag_str} = {lag_periods} per√≠odos\")\n",
        "            else:\n",
        "                # M√©todo 2: Usar frecuencia promedio\n",
        "                lag_periods = int(lag_timedelta / freq_promedio)\n",
        "                print(f\"      ‚úÖ M√©todo 2: {lag_str} = {lag_periods} per√≠odos (freq promedio)\")\n",
        "\n",
        "            # Validar que lag_periods sea razonable\n",
        "            if lag_periods <= 0:\n",
        "                print(f\"      ‚ùå Per√≠odos inv√°lidos ({lag_periods}), saltando lag {lag_str}\")\n",
        "                continue\n",
        "            elif lag_periods > len(df):\n",
        "                print(f\"      ‚ö†Ô∏è  Per√≠odos muy altos ({lag_periods} > {len(df)}), ajustando a {len(df)//4}\")\n",
        "                lag_periods = len(df) // 4\n",
        "\n",
        "            # Crear lag features para cada variable\n",
        "            features_creados_lag = 0\n",
        "            for variable in variables_prioritarias:\n",
        "                if contador_features >= max_features:\n",
        "                    print(f\"   ‚ö†Ô∏è  L√≠mite global de {max_features} features alcanzado\")\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # CORRECCI√ìN: Usar shift() con per√≠odos enteros\n",
        "                    nombre_feature = f\"{variable}_lag_{lag_str}\"\n",
        "                    lag_features[nombre_feature] = df[variable].shift(lag_periods)\n",
        "                    contador_features += 1\n",
        "                    features_creados_lag += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"      ‚ö†Ô∏è  Error con {variable}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"      ‚úÖ Features creados para {lag_str}: {features_creados_lag}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      ‚ùå Error procesando lag {lag_str}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        if contador_features >= max_features:\n",
        "            break\n",
        "\n",
        "    # Crear caracter√≠sticas de diferencias con manejo robusto\n",
        "    print(f\"\\n   üìä Creando caracter√≠sticas de diferencias...\")\n",
        "\n",
        "    diferencias_creadas = 0\n",
        "    for variable in variables_prioritarias[:8]:  # Limitar a primeras 8 variables\n",
        "        if contador_features >= max_features:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Diferencia simple (m√©todo m√°s robusto)\n",
        "            diff_periods_12h = max(1, int(pd.Timedelta('12H') / freq_promedio))\n",
        "            diff_name = f\"{variable}_diff_12H\"\n",
        "            lag_features[diff_name] = df[variable] - df[variable].shift(diff_periods_12h)\n",
        "\n",
        "            # Cambio porcentual\n",
        "            pct_periods_24h = max(1, int(pd.Timedelta('24H') / freq_promedio))\n",
        "            pct_name = f\"{variable}_pct_change_24H\"\n",
        "\n",
        "            # Usar m√©todo robusto para cambio porcentual\n",
        "            previous_values = df[variable].shift(pct_periods_24h)\n",
        "            current_values = df[variable]\n",
        "\n",
        "            # Evitar divisi√≥n por cero\n",
        "            lag_features[pct_name] = np.where(\n",
        "                previous_values != 0,\n",
        "                (current_values - previous_values) / np.abs(previous_values) * 100,\n",
        "                0  # Cuando el valor anterior es 0\n",
        "            )\n",
        "\n",
        "            contador_features += 2\n",
        "            diferencias_creadas += 2\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      ‚ö†Ô∏è  Error con diferencias de {variable}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"      ‚úÖ Caracter√≠sticas de diferencias creadas: {diferencias_creadas}\")\n",
        "\n",
        "    # Limpieza final\n",
        "    print(f\"\\n   üßπ Aplicando limpieza final...\")\n",
        "    features_antes = lag_features.shape[1]\n",
        "\n",
        "    # Eliminar columnas con demasiados NaN\n",
        "    threshold_nan = int(0.8 * len(lag_features))  # Permitir hasta 80% de NaN\n",
        "    lag_features = lag_features.dropna(axis=1, thresh=threshold_nan)\n",
        "\n",
        "    # Eliminar columnas constantes\n",
        "    for col in lag_features.columns:\n",
        "        if lag_features[col].nunique() <= 1:\n",
        "            lag_features = lag_features.drop(col, axis=1)\n",
        "\n",
        "    features_despues = lag_features.shape[1]\n",
        "    if features_antes != features_despues:\n",
        "        print(f\"      üóëÔ∏è  Eliminados {features_antes - features_despues} features problem√°ticos\")\n",
        "\n",
        "    print(f\"‚úÖ Lag features creadas exitosamente: {contador_features} features totales\")\n",
        "    print(f\"üìê Dimensiones finales: {lag_features.shape[0]:,} √ó {lag_features.shape[1]}\")\n",
        "\n",
        "    # Informaci√≥n de calidad\n",
        "    if lag_features.shape[1] > 0:\n",
        "        completitud = (lag_features.count().sum() / (lag_features.shape[0] * lag_features.shape[1])) * 100\n",
        "        print(f\"üìà Completitud promedio: {completitud:.1f}%\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  No se generaron lag features v√°lidas\")\n",
        "\n",
        "    return lag_features\n",
        "\n",
        "# Ejecutar creaci√≥n de lag features CORREGIDA\n",
        "print(\"\\n‚è™ INICIANDO CREACI√ìN DE LAG FEATURES CORREGIDA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    lag_features = crear_lag_features(\n",
        "        df=df,\n",
        "        lags=['2H', '12H', '48H'],\n",
        "        max_features=75\n",
        "    )\n",
        "\n",
        "    if not lag_features.empty:\n",
        "        print(f\"\\nüíæ Memoria lag features: {lag_features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "        print(f\"üéØ LAG FEATURES CREADAS EXITOSAMENTE\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Dataset de lag features vac√≠o - usando DataFrame b√°sico como fallback\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creando lag features: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    lag_features = pd.DataFrame(index=df.index)  # DataFrame vac√≠o como fallback\n",
        "    print(f\"‚ö†Ô∏è  Usando fallback - DataFrame vac√≠o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fulSkkRkUGOU"
      },
      "source": [
        "## 3. üéØ Consolidaci√≥n y Preparaci√≥n del Dataset Final\n",
        "\n",
        "### üîó Integraci√≥n de Caracter√≠sticas\n",
        "\n",
        "En esta fase cr√≠tica consolidamos todas las caracter√≠sticas creadas en un dataset unificado, optimizado para el entrenamiento de modelos de Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W0X61rIVUGOU",
        "outputId": "e9a614ed-74a3-41ca-d2e8-9fdbd49ccd39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Consolidando dataset final de caracter√≠sticas (MODO COMPLETO)...\n",
            "============================================================\n",
            "üìä Variables originales incluidas: 8 (Todas)\n",
            "üìà Rolling features agregadas: 96 (Todas las v√°lidas)\n",
            "‚è™ Lag features agregadas: 40 (Todas las v√°lidas)\n",
            "\n",
            "üßπ Limpieza final del dataset...\n",
            "\n",
            "üìä DATASET FINAL CONSOLIDADO:\n",
            "   üìê Dimensiones: 20,401 filas √ó 144 columnas\n",
            "   üíæ Memoria: 11.4 MB\n",
            "\n",
            "‚úÖ Calidad del dataset final: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Consolidaci√≥n final del dataset de caracter√≠sticas - SIN RECORTES\n",
        "print(\"üîó Consolidando dataset final de caracter√≠sticas (MODO COMPLETO)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    # 1. Seleccionar variables originales (SIN L√çMITE)\n",
        "    # Antes: variables_numericas[:25] -> Ahora: Todas\n",
        "    variables_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    variables_importantes = variables_numericas\n",
        "\n",
        "    # Crear dataset base\n",
        "    dataset_final = df[variables_importantes].copy()\n",
        "    print(f\"üìä Variables originales incluidas: {dataset_final.shape[1]} (Todas)\")\n",
        "\n",
        "    # 2. Agregar Rolling Features (SIN L√çMITE DE CANTIDAD)\n",
        "    if not rolling_features.empty:\n",
        "        # Filtrar rolling features con variabilidad significativa (para no meter basura)\n",
        "        # Mantenemos el filtro de columnas vac√≠as, pero quitamos el recorte [:80]\n",
        "        rolling_features_filtered = rolling_features.dropna(axis=1, thresh=int(0.7 * len(rolling_features)))\n",
        "        rolling_features_filtered = rolling_features_filtered.select_dtypes(include=[np.number])\n",
        "\n",
        "        features_con_variacion = []\n",
        "        for col in rolling_features_filtered.columns:\n",
        "            if rolling_features_filtered[col].std() > 1e-6:\n",
        "                features_con_variacion.append(col)\n",
        "\n",
        "        # CAMBIO AQU√ç: Se elimin√≥ [:80] para incluir todas\n",
        "        rolling_features_final = rolling_features_filtered[features_con_variacion]\n",
        "        dataset_final = pd.concat([dataset_final, rolling_features_final], axis=1)\n",
        "        print(f\"üìà Rolling features agregadas: {rolling_features_final.shape[1]} (Todas las v√°lidas)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  No se agregaron rolling features (dataset vac√≠o)\")\n",
        "\n",
        "    # 3. Agregar Lag Features (SIN L√çMITE DE CANTIDAD)\n",
        "    if not lag_features.empty:\n",
        "        # Mantenemos filtro de calidad b√°sico, pero quitamos el recorte [:40]\n",
        "        lag_features_filtered = lag_features.dropna(axis=1, thresh=int(0.5 * len(lag_features)))\n",
        "        lag_features_filtered = lag_features_filtered.select_dtypes(include=[np.number])\n",
        "\n",
        "        features_con_variacion = []\n",
        "        for col in lag_features_filtered.columns:\n",
        "            if lag_features_filtered[col].std() > 1e-6:\n",
        "                features_con_variacion.append(col)\n",
        "\n",
        "        # CAMBIO AQU√ç: Se elimin√≥ [:40] para incluir todas\n",
        "        lag_features_final = lag_features_filtered[features_con_variacion]\n",
        "        dataset_final = pd.concat([dataset_final, lag_features_final], axis=1)\n",
        "        print(f\"‚è™ Lag features agregadas: {lag_features_final.shape[1]} (Todas las v√°lidas)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  No se agregaron lag features (dataset vac√≠o)\")\n",
        "\n",
        "    print(f\"\\nüßπ Limpieza final del dataset...\")\n",
        "\n",
        "    # Eliminar columnas completamente nulas\n",
        "    columnas_antes = dataset_final.shape[1]\n",
        "    dataset_final = dataset_final.dropna(axis=1, how='all')\n",
        "\n",
        "    # Eliminar features pr√°cticamente constantes (varianza 0)\n",
        "    # NOTA: Esto es necesario para que el modelo LSTM no falle, pero solo borra lo que es plano.\n",
        "    for col in dataset_final.select_dtypes(include=[np.number]).columns:\n",
        "        if dataset_final[col].std() < 1e-10:\n",
        "            dataset_final = dataset_final.drop(col, axis=1)\n",
        "\n",
        "    columnas_despues = dataset_final.shape[1]\n",
        "    if columnas_antes != columnas_despues:\n",
        "        print(f\"   üóëÔ∏è  Eliminadas {columnas_antes - columnas_despues} columnas constantes/vac√≠as\")\n",
        "\n",
        "    # Convertir a float32 para optimizar memoria\n",
        "    numeric_columns = dataset_final.select_dtypes(include=[np.number]).columns\n",
        "    dataset_final[numeric_columns] = dataset_final[numeric_columns].astype(np.float32)\n",
        "\n",
        "    # Informaci√≥n final del dataset\n",
        "    print(f\"\\nüìä DATASET FINAL CONSOLIDADO:\")\n",
        "    print(f\"   üìê Dimensiones: {dataset_final.shape[0]:,} filas √ó {dataset_final.shape[1]} columnas\")\n",
        "    print(f\"   üíæ Memoria: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "    # Verificar calidad del dataset final\n",
        "    completitud_final = (dataset_final.count().sum() / (dataset_final.shape[0] * dataset_final.shape[1])) * 100\n",
        "    print(f\"\\n‚úÖ Calidad del dataset final: {completitud_final:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en consolidaci√≥n del dataset: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    dataset_final = df.select_dtypes(include=[np.number]).copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtAsEkc7UGOU"
      },
      "source": [
        "## 3.5. Creaci√≥n de Variable Objetivo\n",
        "\n",
        "### Etiquetado de Fallas con Ventana de Pre-falla\n",
        "\n",
        "Esta es la fase cr√≠tica donde convertimos los eventos de mantenimiento en la variable objetivo binaria 'falla'. Utilizamos una ventana de 7 d√≠as antes de cada evento para marcar las muestras como positivas (falla inminente)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UJFGLWVUUGOU",
        "outputId": "341ee99a-eb2d-4c82-d300-36a6784c6d2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç DIAGN√ìSTICO DEL PROBLEMA\n",
            "============================================================\n",
            "1. Verificando estado actual del dataset:\n",
            "   Dimensiones: (20401, 144)\n",
            "   Columnas disponibles: 144\n",
            "   Variable 'falla' presente: NO\n",
            "   Primeras 5 columnas: ['pres_aceite_comp', 'rpm', 'presion_aceite_motor', 'presion_agua', 'presion_carter']\n",
            "   √öltimas 5 columnas: ['temp_aceite_motor_pct_change_24H', 'temp_agua_motor_diff_12H', 'temp_agua_motor_pct_change_24H', 'temp_mult_adm_izq_diff_12H', 'temp_mult_adm_izq_pct_change_24H']\n",
            "\n",
            "2. Verificando eventos disponibles:\n",
            "   Variable eventos_utilizables existe: S√ç\n",
            "   Tipo: <class 'pandas.core.series.Series'>\n",
            "   Cantidad de eventos: 8\n",
            "   Primer evento: 2023-03-15 00:00:00\n",
            "   √öltimo evento: 2025-03-01 00:00:00\n",
            "\n",
            "3. Verificando compatibilidad temporal:\n",
            "   Compatibilidad temporal: True\n",
            "\n",
            "4. Recreando eventos utilizables...\n",
            "   Dataset de eventos disponible: S√ç (8 filas)\n",
            "   Eventos con fecha v√°lida: 8\n",
            "   Eventos en rango temporal: 8\n",
            "\n",
            "5. Creando variable objetivo FORZADAMENTE...\n",
            "   Estado antes: (20401, 144)\n",
            "   Iniciando creaci√≥n con 8 eventos...\n",
            "     ‚úÖ Evento 1: 2023-03-15 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 2: 2023-06-20 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 3: 2023-09-10 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 4: 2023-12-05 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 5: 2024-02-15 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 6: 2024-05-20 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 7: 2024-10-10 00:00:00 -> 169 muestras\n",
            "     ‚úÖ Evento 8: 2025-03-01 00:00:00 -> 169 muestras\n",
            "\n",
            "   üìä RESULTADO DE CREACI√ìN:\n",
            "     üî¥ Muestras positivas: 1,352 (6.63%)\n",
            "     üîµ Muestras negativas: 19,049 (93.37%)\n",
            "     ‚úÖ Eventos procesados exitosamente: 8/8\n",
            "   Estado despu√©s: (20401, 145)\n",
            "   ‚úÖ √âXITO: Variable 'falla' creada correctamente\n",
            "   üìä Distribuci√≥n: {0: np.int64(19049), 1: np.int64(1352)}\n",
            "   üéØ Columna 'falla' confirmada en posici√≥n: 144\n",
            "\n",
            "6. VERIFICACI√ìN FINAL:\n",
            "   ‚úÖ Dimensiones finales: (20401, 145)\n",
            "   ‚úÖ Variable 'falla' presente: S√ç\n",
            "   ‚úÖ √öltimas 3 columnas: ['temp_mult_adm_izq_diff_12H', 'temp_mult_adm_izq_pct_change_24H', 'falla']\n",
            "   ‚úÖ Tipo de datos 'falla': int64\n",
            "   ‚úÖ Valores √∫nicos: [np.int64(0), np.int64(1)]\n",
            "   ‚úÖ Conteo de valores: {0: np.int64(19049), 1: np.int64(1352)}\n",
            "\n",
            "üéâ VARIABLE OBJETIVO CONFIRMADA - LISTO PARA GUARDADO\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# DEBUG Y CREACI√ìN FORZADA DE VARIABLE OBJETIVO\n",
        "print(\"üîç DIAGN√ìSTICO DEL PROBLEMA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. VERIFICAR ESTADO ACTUAL DEL DATASET\n",
        "print(\"1. Verificando estado actual del dataset:\")\n",
        "print(f\"   Dimensiones: {dataset_final.shape}\")\n",
        "print(f\"   Columnas disponibles: {len(dataset_final.columns)}\")\n",
        "print(f\"   Variable 'falla' presente: {'S√ç' if 'falla' in dataset_final.columns else 'NO'}\")\n",
        "print(f\"   Primeras 5 columnas: {list(dataset_final.columns[:5])}\")\n",
        "print(f\"   √öltimas 5 columnas: {list(dataset_final.columns[-5:])}\")\n",
        "\n",
        "# 2. VERIFICAR EVENTOS DISPONIBLES\n",
        "print(f\"\\n2. Verificando eventos disponibles:\")\n",
        "if 'eventos_utilizables' in locals():\n",
        "    print(f\"   Variable eventos_utilizables existe: S√ç\")\n",
        "    print(f\"   Tipo: {type(eventos_utilizables)}\")\n",
        "    print(f\"   Cantidad de eventos: {len(eventos_utilizables) if eventos_utilizables is not None else 0}\")\n",
        "    if len(eventos_utilizables) > 0:\n",
        "        print(f\"   Primer evento: {eventos_utilizables.iloc[0]}\")\n",
        "        print(f\"   √öltimo evento: {eventos_utilizables.iloc[-1]}\")\n",
        "else:\n",
        "    print(f\"   Variable eventos_utilizables: NO EXISTE\")\n",
        "\n",
        "# 3. VERIFICAR COMPATIBILIDAD TEMPORAL\n",
        "print(f\"\\n3. Verificando compatibilidad temporal:\")\n",
        "if 'compatibilidad_temporal' in locals():\n",
        "    print(f\"   Compatibilidad temporal: {compatibilidad_temporal}\")\n",
        "else:\n",
        "    print(f\"   Variable compatibilidad_temporal: NO DEFINIDA\")\n",
        "\n",
        "# 4. RECREAR EVENTOS UTILIZABLES SI ES NECESARIO\n",
        "print(f\"\\n4. Recreando eventos utilizables...\")\n",
        "try:\n",
        "    # Verificar si tenemos df_eventos\n",
        "    if 'df_eventos' in locals() and df_eventos is not None:\n",
        "        print(f\"   Dataset de eventos disponible: S√ç ({len(df_eventos)} filas)\")\n",
        "\n",
        "        # Recrear eventos utilizables\n",
        "        if 'fecha_evento' in df_eventos.columns:\n",
        "            eventos_validos = df_eventos['fecha_evento'].dropna()\n",
        "            print(f\"   Eventos con fecha v√°lida: {len(eventos_validos)}\")\n",
        "\n",
        "            # Filtrar eventos dentro del rango del dataset principal\n",
        "            sensor_start, sensor_end = dataset_final.index.min(), dataset_final.index.max()\n",
        "            eventos_en_rango = eventos_validos[\n",
        "                (eventos_validos >= sensor_start) & (eventos_validos <= sensor_end)\n",
        "            ]\n",
        "\n",
        "            print(f\"   Eventos en rango temporal: {len(eventos_en_rango)}\")\n",
        "            eventos_utilizables = eventos_en_rango\n",
        "\n",
        "        else:\n",
        "            print(f\"   ERROR: Columna 'fecha_evento' no encontrada\")\n",
        "            eventos_utilizables = pd.Series(dtype='datetime64[ns]')\n",
        "    else:\n",
        "        print(f\"   Dataset de eventos NO disponible\")\n",
        "        eventos_utilizables = pd.Series(dtype='datetime64[ns]')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ERROR recreando eventos: {str(e)}\")\n",
        "    eventos_utilizables = pd.Series(dtype='datetime64[ns]')\n",
        "\n",
        "# 5. CREAR VARIABLE OBJETIVO DE FORMA FORZADA\n",
        "print(f\"\\n5. Creando variable objetivo FORZADAMENTE...\")\n",
        "\n",
        "def crear_variable_falla_robusta(dataset, eventos, ventana_dias=7):\n",
        "    \"\"\"\n",
        "    Versi√≥n robusta de creaci√≥n de variable objetivo\n",
        "    \"\"\"\n",
        "    print(f\"   Iniciando creaci√≥n con {len(eventos)} eventos...\")\n",
        "\n",
        "    # Crear columna 'falla' inicializada en 0\n",
        "    dataset = dataset.copy()\n",
        "    dataset['falla'] = 0\n",
        "\n",
        "    if len(eventos) == 0:\n",
        "        print(f\"   ‚ö†Ô∏è  Sin eventos disponibles - todas las muestras como normales\")\n",
        "        return dataset\n",
        "\n",
        "    ventana_timedelta = pd.Timedelta(days=ventana_dias)\n",
        "    muestras_marcadas = 0\n",
        "    eventos_procesados = 0\n",
        "\n",
        "    for fecha_evento in eventos:\n",
        "        try:\n",
        "            # Calcular ventana de pre-falla\n",
        "            inicio_ventana = fecha_evento - ventana_timedelta\n",
        "            fin_ventana = fecha_evento\n",
        "\n",
        "            # Crear m√°scara para el per√≠odo de pre-falla\n",
        "            mask = (dataset.index >= inicio_ventana) & (dataset.index <= fin_ventana)\n",
        "            muestras_en_ventana = mask.sum()\n",
        "\n",
        "            if muestras_en_ventana > 0:\n",
        "                dataset.loc[mask, 'falla'] = 1\n",
        "                muestras_marcadas += muestras_en_ventana\n",
        "                eventos_procesados += 1\n",
        "                print(f\"     ‚úÖ Evento {eventos_procesados}: {fecha_evento} -> {muestras_en_ventana} muestras\")\n",
        "            else:\n",
        "                print(f\"     ‚ö†Ô∏è  Evento {fecha_evento}: Sin muestras en ventana\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"     ‚ùå Error procesando evento {fecha_evento}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Estad√≠sticas finales\n",
        "    total_muestras = len(dataset)\n",
        "    muestras_positivas = (dataset['falla'] == 1).sum()\n",
        "    muestras_negativas = (dataset['falla'] == 0).sum()\n",
        "    porcentaje_positivas = (muestras_positivas / total_muestras) * 100\n",
        "\n",
        "    print(f\"\\n   üìä RESULTADO DE CREACI√ìN:\")\n",
        "    print(f\"     üî¥ Muestras positivas: {muestras_positivas:,} ({porcentaje_positivas:.2f}%)\")\n",
        "    print(f\"     üîµ Muestras negativas: {muestras_negativas:,} ({100-porcentaje_positivas:.2f}%)\")\n",
        "    print(f\"     ‚úÖ Eventos procesados exitosamente: {eventos_procesados}/{len(eventos)}\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Ejecutar creaci√≥n robusta\n",
        "try:\n",
        "    print(f\"   Estado antes: {dataset_final.shape}\")\n",
        "    dataset_final = crear_variable_falla_robusta(dataset_final, eventos_utilizables)\n",
        "    print(f\"   Estado despu√©s: {dataset_final.shape}\")\n",
        "\n",
        "    # VERIFICACI√ìN CR√çTICA\n",
        "    if 'falla' in dataset_final.columns:\n",
        "        print(f\"   ‚úÖ √âXITO: Variable 'falla' creada correctamente\")\n",
        "        distribuci√≥n = dataset_final['falla'].value_counts().sort_index()\n",
        "        print(f\"   üìä Distribuci√≥n: {dict(distribuci√≥n)}\")\n",
        "        print(f\"   üéØ Columna 'falla' confirmada en posici√≥n: {list(dataset_final.columns).index('falla')}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå ERROR: Variable 'falla' A√öN no se cre√≥\")\n",
        "        # Crear manualmente como √∫ltimo recurso\n",
        "        print(f\"   üö® CREACI√ìN MANUAL DE EMERGENCIA...\")\n",
        "        dataset_final['falla'] = 0\n",
        "        print(f\"   ‚úÖ Variable 'falla' creada manualmente (todas negativas)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå ERROR en creaci√≥n robusta: {str(e)}\")\n",
        "    # Crear variable por defecto\n",
        "    dataset_final['falla'] = 0\n",
        "    print(f\"   üö® Variable por defecto creada\")\n",
        "\n",
        "# 6. VERIFICACI√ìN FINAL ANTES DEL GUARDADO\n",
        "print(f\"\\n6. VERIFICACI√ìN FINAL:\")\n",
        "print(f\"   ‚úÖ Dimensiones finales: {dataset_final.shape}\")\n",
        "print(f\"   ‚úÖ Variable 'falla' presente: {'S√ç' if 'falla' in dataset_final.columns else 'NO'}\")\n",
        "print(f\"   ‚úÖ √öltimas 3 columnas: {list(dataset_final.columns[-3:])}\")\n",
        "\n",
        "if 'falla' in dataset_final.columns:\n",
        "    print(f\"   ‚úÖ Tipo de datos 'falla': {dataset_final['falla'].dtype}\")\n",
        "    print(f\"   ‚úÖ Valores √∫nicos: {sorted(dataset_final['falla'].unique())}\")\n",
        "    print(f\"   ‚úÖ Conteo de valores: {dict(dataset_final['falla'].value_counts())}\")\n",
        "    print(f\"\\nüéâ VARIABLE OBJETIVO CONFIRMADA - LISTO PARA GUARDADO\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå VARIABLE OBJETIVO FALLA - REVISAR MANUALMENTE\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3Z3_gfwUGOV"
      },
      "source": [
        "## 4. üíæ Guardado y Finalizaci√≥n\n",
        "\n",
        "### üìÅ Persistencia del Dataset de Caracter√≠sticas\n",
        "\n",
        "Guardamos el dataset final optimizado en m√∫ltiples formatos para garantizar compatibilidad y eficiencia en las fases posteriores de modelado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e7ykh0rsUGOV",
        "outputId": "f0ddf489-99cd-43da-db85-893e0d541d0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Guardando dataset final de caracter√≠sticas...\n",
            "============================================================\n",
            "‚úÖ Parquet guardado: features_dataset_20251210_000020.parquet (12.2 MB)\n",
            "‚úÖ CSV guardado: features_dataset_20251210_000020.csv (28.1 MB)\n",
            "‚úÖ Metadatos guardados: features_dataset_20251210_000020_metadata.txt\n",
            "‚úÖ Estad√≠sticas guardadas: features_dataset_20251210_000020_statistics.csv\n",
            "\n",
            "üéØ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\n",
            "üìÅ Archivos generados en data/processed:\n",
            "   üì¶ features_dataset_20251210_000020.parquet - Dataset principal (comprimido)\n",
            "   üìÑ features_dataset_20251210_000020.csv - Dataset principal (CSV)\n",
            "   üìã features_dataset_20251210_000020_metadata.txt - Metadatos completos\n",
            "   üìä features_dataset_20251210_000020_statistics.csv - Estad√≠sticas descriptivas\n",
            "üéâ Ingenier√≠a de Caracter√≠sticas finalizado exitosamente!\n"
          ]
        }
      ],
      "source": [
        "# Guardado del dataset final con metadatos completos\n",
        "print(\"üíæ Guardando dataset final de caracter√≠sticas...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configurar rutas de salida\n",
        "output_dir = ruta_processed\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Nombres de archivos de salida\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "base_name = f'features_dataset_{timestamp}'\n",
        "\n",
        "try:\n",
        "    # 1. Guardar en formato Parquet (optimizado)\n",
        "    archivo_parquet = output_dir / f'{base_name}.parquet'\n",
        "    dataset_final.to_parquet(archivo_parquet, engine='pyarrow', compression='snappy')\n",
        "    tama√±o_parquet = archivo_parquet.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ Parquet guardado: {archivo_parquet.name} ({tama√±o_parquet:.1f} MB)\")\n",
        "\n",
        "    # 2. Guardar en formato CSV (compatibilidad)\n",
        "    archivo_csv = output_dir / f'{base_name}.csv'\n",
        "    dataset_final.to_csv(archivo_csv, encoding='utf-8')\n",
        "    tama√±o_csv = archivo_csv.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ CSV guardado: {archivo_csv.name} ({tama√±o_csv:.1f} MB)\")\n",
        "\n",
        "    # 3. Generar archivo de metadatos\n",
        "    archivo_metadata = output_dir / f'{base_name}_metadata.txt'\n",
        "    with open(archivo_metadata, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"METADATOS DEL DATASET DE CARACTER√çSTICAS\\n\")\n",
        "        f.write(f\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Notebook: 03_feature_engineering_b.ipynb\\n\")\n",
        "        f.write(f\"\\nDimensiones: {dataset_final.shape[0]:,} √ó {dataset_final.shape[1]}\\n\")\n",
        "        f.write(f\"Per√≠odo temporal: {dataset_final.index.min()} ‚Üí {dataset_final.index.max()}\\n\")\n",
        "        f.write(f\"Memoria utilizada: {dataset_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\\n\")\n",
        "        f.write(f\"\\nTipos de caracter√≠sticas:\\n\")\n",
        "        f.write(f\"  - Variables originales: {len(variables_importantes)}\\n\")\n",
        "        if 'rolling_features_final' in locals():\n",
        "            f.write(f\"  - Rolling features: {rolling_features_final.shape[1]}\\n\")\n",
        "        if 'lag_features_final' in locals():\n",
        "            f.write(f\"  - Lag features: {lag_features_final.shape[1]}\\n\")\n",
        "        f.write(f\"\\nEventos de mantenimiento:\\n\")\n",
        "        if eventos_utilizables is not None and len(eventos_utilizables) > 0:\n",
        "            f.write(f\"  - Eventos utilizables: {len(eventos_utilizables)}\\n\")\n",
        "            f.write(f\"  - Compatibilidad temporal: {'Confirmada' if compatibilidad_temporal else 'No confirmada'}\\n\")\n",
        "        else:\n",
        "            f.write(f\"  - Sin eventos utilizables identificados\\n\")\n",
        "\n",
        "        f.write(f\"\\nLista de columnas:\\n\")\n",
        "        for i, col in enumerate(dataset_final.columns, 1):\n",
        "            f.write(f\"  {i:3d}. {col}\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Metadatos guardados: {archivo_metadata.name}\")\n",
        "\n",
        "    # 4. Generar resumen estad√≠stico\n",
        "    archivo_stats = output_dir / f'{base_name}_statistics.csv'\n",
        "    stats_desc = dataset_final.describe()\n",
        "    stats_desc.to_csv(archivo_stats, encoding='utf-8')\n",
        "    print(f\"‚úÖ Estad√≠sticas guardadas: {archivo_stats.name}\")\n",
        "\n",
        "    # Resumen final\n",
        "    print(f\"\\nüéØ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
        "    print(f\"üìÅ Archivos generados en {output_dir}:\")\n",
        "    print(f\"   üì¶ {base_name}.parquet - Dataset principal (comprimido)\")\n",
        "    print(f\"   üìÑ {base_name}.csv - Dataset principal (CSV)\")\n",
        "    print(f\"   üìã {base_name}_metadata.txt - Metadatos completos\")\n",
        "    print(f\"   üìä {base_name}_statistics.csv - Estad√≠sticas descriptivas\")\n",
        "\n",
        "    print(f\"üéâ Ingenier√≠a de Caracter√≠sticas finalizado exitosamente!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al guardar dataset: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Guardado de emergencia\n",
        "    backup_file = output_dir / f'features_dataset_emergency_backup.csv'\n",
        "    dataset_final.to_csv(backup_file)\n",
        "    print(f\"üíæ Guardado de emergencia: {backup_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# CORRECCI√ìN DE NOMBRE PARA EL SIGUIENTE NOTEBOOK\n",
        "# -------------------------------------------------------------------------\n",
        "import shutil\n",
        "\n",
        "# 1. Definir la ruta exacta que busca el Notebook 04\n",
        "archivo_requerido_parquet = ruta_processed / 'featured_dataset_with_target.parquet'\n",
        "archivo_requerido_csv = ruta_processed / 'featured_dataset_with_target.csv'\n",
        "\n",
        "print(f\"üîß Guardando copias estandarizadas para el Notebook 04...\")\n",
        "\n",
        "try:\n",
        "    # Opci√≥n A: Guardar directamente desde el DataFrame en memoria (M√°s seguro)\n",
        "    if 'dataset_final' in locals():\n",
        "        dataset_final.to_parquet(archivo_requerido_parquet, engine='pyarrow', compression='snappy')\n",
        "        dataset_final.to_csv(archivo_requerido_csv)\n",
        "        print(f\"   ‚úÖ Archivo creado: {archivo_requerido_parquet}\")\n",
        "        print(f\"   ‚úÖ Archivo creado: {archivo_requerido_csv}\")\n",
        "\n",
        "    # Opci√≥n B: Si ya cerraste pandas, renombramos el √∫ltimo generado\n",
        "    else:\n",
        "        # (Esto solo se ejecuta si dataset_final no est√° en memoria)\n",
        "        print(\"   ‚ö†Ô∏è DataFrame no encontrado en memoria, buscando √∫ltimo archivo generado...\")\n",
        "        # L√≥gica de respaldo no necesaria si acabas de correr todo.\n",
        "        pass\n",
        "\n",
        "    print(\"\\nüöÄ LISTO: Ahora el Notebook 04 encontrar√° el archivo 'featured_dataset_with_target.parquet' sin problemas.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al guardar el archivo estandarizado: {str(e)}\")"
      ],
      "metadata": {
        "id": "rJdIrVsq45pZ",
        "outputId": "e08ad788-1f11-4c55-823d-96eb72a8debd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Guardando copias estandarizadas para el Notebook 04...\n",
            "   ‚úÖ Archivo creado: data/processed/featured_dataset_with_target.parquet\n",
            "   ‚úÖ Archivo creado: data/processed/featured_dataset_with_target.csv\n",
            "\n",
            "üöÄ LISTO: Ahora el Notebook 04 encontrar√° el archivo 'featured_dataset_with_target.parquet' sin problemas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asRy5lRJUGOW"
      },
      "source": [
        "## üìã Resumen del Feature Engineering\n",
        "\n",
        "### üéØ Logros Alcanzados\n",
        "\n",
        "1. **‚úÖ Carga de datos exitosa** - Integraci√≥n de series temporales y historial de eventos\n",
        "2. **‚úÖ Validaci√≥n temporal** - Confirmaci√≥n de compatibilidad entre datasets\n",
        "3. **‚úÖ Caracter√≠sticas temporales** - Creaci√≥n de rolling features y lag features\n",
        "4. **‚úÖ Dataset optimizado** - Consolidaci√≥n y optimizaci√≥n de memoria\n",
        "5. **‚úÖ Persistencia completa** - Guardado en m√∫ltiples formatos con metadatos\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}